{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Mario & Bowser Game**"
      ],
      "metadata": {
        "id": "Uo5_ktq-nHN1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jftZ_SfOnCQz"
      },
      "outputs": [],
      "source": [
        "'''Player class. This class stores information relevant for a player in the game,\n",
        "namely, Mario or Bowser. The information stored is the name of the player and a list of\n",
        "states in which the player can be.\n",
        "'''\n",
        "\n",
        "\n",
        "class player:\n",
        "    # Initializer\n",
        "    def __init__(self,name='player',states=[]):\n",
        "        self.name = name\n",
        "        self.states = states\n",
        "        if not type(states) is list:\n",
        "            self.states = []\n",
        "\n",
        "    # A method for adding a legal state to the player's state list.\n",
        "    def add_state(self,state):\n",
        "        if state in self.states:\n",
        "            return\n",
        "        self.states.append(state)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''State class. This class stores information about a state in the game.\n",
        "Each state has a number of valid actions. The actions are stored in the\n",
        "list actions and the resulting state per action are stored in the dict outcomes.\n",
        "'''\n",
        "\n",
        "\n",
        "class state:\n",
        "    # Initializer\n",
        "    def __init__(self,name=0,actions=None,outcomes=None):\n",
        "        self.name = name\n",
        "        self.actions = actions\n",
        "        if not type(actions) is list:\n",
        "            self.actions = []\n",
        "        self.outcomes = outcomes\n",
        "        if not type(outcomes) is dict:\n",
        "            self.outcomes = {}\n",
        "\n",
        "    # A setter method the outcome of a given action.\n",
        "    def add_outcome(self,action,outcome):\n",
        "        if not action in self.actions:\n",
        "            self.actions.append(action)\n",
        "\n",
        "        self.outcomes[action] = outcome"
      ],
      "metadata": {
        "id": "AKbOjP1cnRkd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Mario vs. Bowser\n",
        "Final project in the course: Multiagent Systems and Distributed Systems\n",
        "\n",
        "(C) Copyright 5783\n",
        "Udi Adler\n",
        "Chaim Schendowich\n",
        "\n",
        "Game class. Contains all necessary information about the game structure and rules.\n",
        "More specifically:\n",
        "    1. Definition of the game states (1-6).\n",
        "    2. Definition of the players (1-2).\n",
        "    3. Definition of the transition probabilities (P).\n",
        "    4. Definition of the transition rewards (R).\n",
        "'''\n",
        "\n",
        "\n",
        "class game:\n",
        "    # Initializer\n",
        "    # A preferences list is a required argument.\n",
        "    def __init__(self,prefs):\n",
        "        self.prefs = prefs\n",
        "        self.initialize_board()\n",
        "\n",
        "        # Initialize probabilities for single player stochatic transitions.\n",
        "        self.p1 = {}\n",
        "        self.p1[(1,\"up\")] = {}\n",
        "        self.p1[(1,\"up\")][4] = self.prefs['P[1|u]']/(self.prefs['P[1|u]']+self.prefs['P[1|!u]'])\n",
        "        self.p1[(1,\"up\")][1] = self.prefs['P[1|!u]']/(self.prefs['P[1|u]']+self.prefs['P[1|!u]'])\n",
        "        self.p1[(3,\"up\")] = {}\n",
        "        self.p1[(3,\"up\")][6] = self.prefs['P[3|u]']/(self.prefs['P[3|u]']+self.prefs['P[3|!u]'])\n",
        "        self.p1[(3,\"up\")][3] = self.prefs['P[3|!u]']/(self.prefs['P[3|u]']+self.prefs['P[3|!u]'])\n",
        "\n",
        "        # Initialize probabilities for two player stochastic transitions.\n",
        "        self.p2 = {}\n",
        "        self.p2[(1,3)] = {}\n",
        "        self.p2[(1,3)][(\"right\",\"left\")] = {}\n",
        "        self.p2[(1,3)][(\"right\",\"left\")][1] = self.prefs['P[(1,3)(r,l)|l]']/(self.prefs['P[(1,3)(r,l)|r]']+self.prefs['P[(1,3)(r,l)|l]'])\n",
        "        self.p2[(1,3)][(\"right\",\"left\")][2] = self.prefs['P[(1,3)(r,l)|r]']/(self.prefs['P[(1,3)(r,l)|r]']+self.prefs['P[(1,3)(r,l)|l]'])\n",
        "        self.p2[(3,1)] = {}\n",
        "        self.p2[(3,1)][(\"left\",\"right\")] = {}\n",
        "        self.p2[(3,1)][(\"left\",\"right\")][2] = self.prefs['P[(1,3)(r,l)|l]']/(self.prefs['P[(1,3)(r,l)|r]']+self.prefs['P[(1,3)(r,l)|l]'])\n",
        "        self.p2[(3,1)][(\"left\",\"right\")][3] = self.prefs['P[(1,3)(r,l)|r]']/(self.prefs['P[(1,3)(r,l)|r]']+self.prefs['P[(1,3)(r,l)|l]'])\n",
        "        self.p2[(4,6)] = {}\n",
        "        self.p2[(4,6)][(\"right\",\"left\")] = {}\n",
        "        self.p2[(4,6)][(\"right\",\"left\")][4] = self.prefs['P[(4,6)(r,l)|l]']/(self.prefs['P[(4,6)(r,l)|r]']+self.prefs['P[(4,6)(r,l)|l]'])\n",
        "        self.p2[(4,6)][(\"right\",\"left\")][5] = self.prefs['P[(4,6)(r,l)|r]']/(self.prefs['P[(4,6)(r,l)|r]']+self.prefs['P[(4,6)(r,l)|l]'])\n",
        "        self.p2[(6,4)] = {}\n",
        "        self.p2[(6,4)][(\"left\",\"right\")] = {}\n",
        "        self.p2[(6,4)][(\"left\",\"right\")][5] = self.prefs['P[(4,6)(r,l)|l]']/(self.prefs['P[(4,6)(r,l)|r]']+self.prefs['P[(4,6)(r,l)|l]'])\n",
        "        self.p2[(6,4)][(\"left\",\"right\")][6] = self.prefs['P[(4,6)(r,l)|r]']/(self.prefs['P[(4,6)(r,l)|r]']+self.prefs['P[(4,6)(r,l)|l]'])\n",
        "        self.p2[(4,2)] = {}\n",
        "        self.p2[(4,2)][(\"right\",\"up\")] = {}\n",
        "        self.p2[(4,2)][(\"right\",\"up\")][4] = self.prefs['P[(4,2)(r,u)|u]']/(self.prefs['P[(4,2)(r,u)|r]']+self.prefs['P[(4,2)(r,u)|u]'])\n",
        "        self.p2[(4,2)][(\"right\",\"up\")][5] = self.prefs['P[(4,2)(r,u)|r]']/(self.prefs['P[(4,2)(r,u)|r]']+self.prefs['P[(4,2)(r,u)|u]'])\n",
        "        self.p2[(2,4)] = {}\n",
        "        self.p2[(2,4)][(\"up\",\"right\")] = {}\n",
        "        self.p2[(2,4)][(\"up\",\"right\")][2] = self.prefs['P[(4,2)(r,u)|r]']/(self.prefs['P[(4,2)(r,u)|r]']+self.prefs['P[(4,2)(r,u)|u]'])\n",
        "        self.p2[(2,4)][(\"up\",\"right\")][5] = self.prefs['P[(4,2)(r,u)|u]']/(self.prefs['P[(4,2)(r,u)|r]']+self.prefs['P[(4,2)(r,u)|u]'])\n",
        "        self.p2[(2,6)] = {}\n",
        "        self.p2[(2,6)][(\"up\",\"left\")] = {}\n",
        "        self.p2[(2,6)][(\"up\",\"left\")][2] = self.prefs['P[(2,6)(u,l)|l]']/(self.prefs['P[(2,6)(u,l)|u]']+self.prefs['P[(2,6)(u,l)|l]'])\n",
        "        self.p2[(2,6)][(\"up\",\"left\")][5] = self.prefs['P[(2,6)(u,l)|u]']/(self.prefs['P[(2,6)(u,l)|u]']+self.prefs['P[(2,6)(u,l)|l]'])\n",
        "        self.p2[(6,2)] = {}\n",
        "        self.p2[(6,2)][(\"left\",\"up\")] = {}\n",
        "        self.p2[(6,2)][(\"left\",\"up\")][5] = self.prefs['P[(2,6)(u,l)|l]']/(self.prefs['P[(2,6)(u,l)|u]']+self.prefs['P[(2,6)(u,l)|l]'])\n",
        "        self.p2[(6,2)][(\"left\",\"up\")][6] = self.prefs['P[(2,6)(u,l)|u]']/(self.prefs['P[(2,6)(u,l)|u]']+self.prefs['P[(2,6)(u,l)|l]'])\n",
        "\n",
        "        # Initialize rewards.\n",
        "        self.r = {}\n",
        "        self.r[((1,3),(\"right\",\"left\"),2)] = self.prefs['R[H2H]']\n",
        "        self.r[((3,1),(\"left\",\"right\"),2)] = self.prefs['R[H2H]']\n",
        "        self.r[((1,3),(\"right\",\"left\"),1)] = -self.prefs['R[H2H]']\n",
        "        self.r[((3,1),(\"left\",\"right\"),3)] = -self.prefs['R[H2H]']\n",
        "        self.r[((4,6),(\"right\",\"left\"),5)] = self.prefs['R[H2H]']+self.prefs['R[Win]']\n",
        "        self.r[((6,4),(\"left\",\"right\"),5)] = self.prefs['R[H2H]']+self.prefs['R[Win]']\n",
        "        self.r[((4,6),(\"right\",\"left\"),4)] = -self.prefs['R[H2H]']-self.prefs['R[Win]']\n",
        "        self.r[((6,4),(\"left\",\"right\"),6)] = -self.prefs['R[H2H]']-self.prefs['R[Win]']\n",
        "\n",
        "    # Board initialization method.\n",
        "    def initialize_board(self):\n",
        "        # Initialize states.\n",
        "        state1 = state(1,actions=[\"right\",\"up\"])\n",
        "        state1.add_outcome(\"right\",2)\n",
        "        state1.add_outcome(\"up\",4)\n",
        "        state2 = state(2,actions=[\"up\"])\n",
        "        state2.add_outcome(\"up\",5)\n",
        "        state3 = state(3,actions=[\"left\",\"up\"])\n",
        "        state3.add_outcome(\"left\",2)\n",
        "        state3.add_outcome(\"up\",6)\n",
        "        state4 = state(4,actions=[\"right\"])\n",
        "        state4.add_outcome(\"right\",5)\n",
        "        state5 = state(5,actions=[\"\"])\n",
        "        state6 = state(6,actions=[\"left\"])\n",
        "        state6.add_outcome(\"left\",5)\n",
        "        self.states = [state1,state2,state3,state4,state5,state6]\n",
        "\n",
        "        # Initialize players.\n",
        "        player1 = player(\"Mario\",[1,2,4,5])\n",
        "        player1.start = 1\n",
        "        player2 = player(\"Bowser\",[2,3,5,6])\n",
        "        player2.start = 3\n",
        "        self.players = [player1,player2]\n",
        "\n",
        "    # Boolean method that tests if the transitions of the players are contradictory,\n",
        "    # resulting in a head-to-head situation.\n",
        "    def IsHeadToHead(self,src,act,opp_s,opp_a):\n",
        "        return self.states[src-1].outcomes[act] == self.states[opp_s-1].outcomes[opp_a]\n",
        "\n",
        "    # Transition probability method.\n",
        "    def P(self,src,dest,act,opp_s,opp_a):\n",
        "        # Single player transition.\n",
        "        if (src,act) in self.p1 and \\\n",
        "           dest in self.p1[(src,act)]:\n",
        "            return self.p1[(src,act)][dest]\n",
        "\n",
        "        # Two player transition.\n",
        "        if (src,opp_s) in self.p2 and \\\n",
        "           (act,opp_a) in self.p2[(src,opp_s)] and \\\n",
        "           dest in self.p2[(src,opp_s)][(act,opp_a)]:\n",
        "            return self.p2[(src,opp_s)][(act,opp_a)][dest]\n",
        "\n",
        "        # Legal deterministic action.\n",
        "        if act in self.states[src-1].actions and \\\n",
        "           self.states[src-1].outcomes[act] == dest:\n",
        "            return 1\n",
        "\n",
        "        return 0\n",
        "\n",
        "    # Transition reward method.\n",
        "    def R(self,src,dest,act,opp_s,opp_a):\n",
        "        # Head to head transition.\n",
        "        if ((src,opp_s),(act,opp_a),dest) in self.r:\n",
        "            return self.r[((src,opp_s),(act,opp_a),dest)]\n",
        "\n",
        "        # Player wins transition,\n",
        "        if act in self.states[src-1].actions and \\\n",
        "           self.states[src-1].outcomes[act] == dest and \\\n",
        "           dest == 5:\n",
        "            return self.prefs['R[Win]']\n",
        "\n",
        "        # Opponent wins transition.\n",
        "        if opp_a in self.states[opp_s-1].actions and \\\n",
        "           self.states[opp_s-1].outcomes[opp_a] == 5:\n",
        "            return -self.prefs['R[Win]']\n",
        "\n",
        "        # Anything else.\n",
        "        return 0\n"
      ],
      "metadata": {
        "id": "oioeDL4OngTl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''UI static class. This class has all the static methods necessary for executing the\n",
        "various game options.\n",
        "'''\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "class ui:\n",
        "    # Prints the game menu.\n",
        "    def print_menu():\n",
        "        print(\"------------------------------------------------\")\n",
        "        print(\"Menu\")\n",
        "        print(\"------------------------------------------------\")\n",
        "        print(\"    1. Change preferences\")\n",
        "        print(\"    2. Train agents\")\n",
        "        print(\"    3. Play demo\")\n",
        "        print(\"    4. Exit\")\n",
        "        print(\"------------------------------------------------\")\n",
        "\n",
        "    # Shows the current values of the preferences and allows the user to\n",
        "    # change them at will.\n",
        "    # Preferences that are numbers need a numeric input.\n",
        "    # Preferences that are ratios need an input string of the format \"p:q\".\n",
        "    def update_prefs(prefs):\n",
        "        done = False\n",
        "        keys = ['','R[H2H]','R[Win]','Discount',\\\n",
        "                ('P[(1,3)(r,l)|r]','P[(1,3)(r,l)|l]'),\\\n",
        "                ('P[(4,6)(r,l)|r]','P[(4,6)(r,l)|l]'),\\\n",
        "                ('P[(4,2)(r,u)|r]','P[(4,2)(r,u)|u]'),\\\n",
        "                ('P[(2,6)(u,l)|u]','P[(2,6)(u,l)|l]'),\\\n",
        "                ('P[1|u]','P[1|!u]'),\\\n",
        "                ('P[3|u]','P[3|!u]')\\\n",
        "                ]\n",
        "        while not done:\n",
        "            # Print old values\n",
        "            print(\"Current preferences:\")\n",
        "            print(\"1. Reward head to head:\", \\\n",
        "                  prefs['R[H2H]'])\n",
        "            print(\"2. Reward win:\", \\\n",
        "                  prefs['R[Win]'])\n",
        "            print(\"3. Discount factor:\", \\\n",
        "                  prefs['Discount'])\n",
        "            print(\"4. Ratio head to head on 2 (M:B):\", \\\n",
        "                  str(prefs['P[(1,3)(r,l)|r]']) + \":\" + \\\n",
        "                  str(prefs['P[(1,3)(r,l)|l]']))\n",
        "            print(\"5. Ratio head to head on 5 (M:B):\", \\\n",
        "                  str(prefs['P[(4,6)(r,l)|r]']) + \":\" + \\\n",
        "                  str(prefs['P[(4,6)(r,l)|l]']))\n",
        "            print(\"6. Fence ratio from 4 (M:B):\", \\\n",
        "                  str(prefs['P[(4,2)(r,u)|r]']) + \":\" + \\\n",
        "                  str(prefs['P[(4,2)(r,u)|u]']))\n",
        "            print(\"7. Fence ratio from 6 (M:B):\", \\\n",
        "                  str(prefs['P[(2,6)(u,l)|u]']) + \":\" + \\\n",
        "                  str(prefs['P[(2,6)(u,l)|l]']))\n",
        "            print(\"8. Barrier ratio from 1 (Y:N):\", \\\n",
        "                  str(prefs['P[1|u]']) + \":\" + \\\n",
        "                  str(prefs['P[1|!u]']))\n",
        "            print(\"9. Barrier ratio from 3 (Y:N):\", \\\n",
        "                  str(prefs['P[3|u]']) + \":\" + \\\n",
        "                  str(prefs['P[3|!u]']))\n",
        "            # Prompt user\n",
        "            choice = input(\"Which preference do you want to change (0-exit)? \")\n",
        "            option = int(choice)\n",
        "            if option > 0 and option <= 9:\n",
        "                value = input(\"What is the new value for the preference? \")\n",
        "            if option == 0:                                                 # Exit\n",
        "                done = True\n",
        "            elif option == 1 or option == 2 or option == 3:                 # Numeric\n",
        "                prefs[keys[option]] = float(value)\n",
        "            elif option > 3 and option <= 9:                                # Ratio\n",
        "                pair = value.split(':')\n",
        "                prefs[keys[option][0]] = int(pair[0])\n",
        "                prefs[keys[option][1]] = int(pair[1])\n",
        "            else:                                                           # Bad input\n",
        "                print(\"Invalid choice!\")\n",
        "        return prefs\n",
        "\n",
        "    # Initializes the game and trains the agents\n",
        "    def train_agents(prefs):\n",
        "        # Initialize game\n",
        "        the_game = game(prefs)\n",
        "\n",
        "        # Train agents\n",
        "        agent1 = agent(the_game,1)\n",
        "        agent2 = agent(the_game,2)\n",
        "        agent1.train(num_episodes=10000, opponent=agent2)\n",
        "        agent2.train(num_episodes=10000, opponent=agent1)\n",
        "\n",
        "        # Print training results\n",
        "        agent1.print_optimal_strategies()\n",
        "        agent2.print_optimal_strategies()\n",
        "\n",
        "        # Return game and trained agents\n",
        "        return (the_game,agent1,agent2)\n",
        "\n",
        "    # Demonstrates a simulation of the game.\n",
        "    # The simulation is a number of iterations of the game played according to\n",
        "    # one of two possible strategies for each player:\n",
        "    #     1. Optimal stragegy: the strategy learned in the training process.\n",
        "    #     2. Random strategy: a uniform random possible action is chosen each turn.\n",
        "    # After each iteration the rewards for Mario and Bowser are displayed.\n",
        "    # After the last iteration the total numbers of wins, the average number of turns\n",
        "    # and the total rewards are displayed.\n",
        "    def play_demo(g,agent1,agent2):\n",
        "        # Input demo definitions\n",
        "        num_games = input(\"How many games do you want to simulate? \")\n",
        "        try:\n",
        "            num_games = int(num_games)\n",
        "        except:\n",
        "            print(\"Number of games must be an integer!\")\n",
        "            ui.play_demo(g,agent1,agent2)\n",
        "            return\n",
        "\n",
        "        p1_optimal = input(\"Do you want \" + g.players[0].name + \" to use optimal strategies (Y/N)? \")\n",
        "        p2_optimal = input(\"Do you want \" + g.players[1].name + \" to use optimal strategies (Y/N)? \")\n",
        "\n",
        "        p1_optimal = p1_optimal == 'Y' or p1_optimal == 'y'\n",
        "        p2_optimal = p2_optimal == 'Y' or p2_optimal == 'y'\n",
        "\n",
        "        # Initialize counters and summers\n",
        "        p1_wins = 0\n",
        "        p2_wins = 0\n",
        "        p1_total_reward = 0\n",
        "        p2_total_reward = 0\n",
        "        total_moves = 0\n",
        "\n",
        "        # Demo loop\n",
        "        for num_game in range(num_games):\n",
        "            print(str(num_game+1) + '.', end = '')\n",
        "            q1 = False  # Player 1 won\n",
        "            q2 = False  # Player 2 won\n",
        "            moves = 0\n",
        "            p1_game_reward = 0\n",
        "            p2_game_reward = 0\n",
        "            game_state = (1,3)  # Mario always starts at state 1, Bowser at state 3\n",
        "            print(game_state, end = '')\n",
        "            # Game loop\n",
        "            while not q1 and not q2:\n",
        "                rand_result1 = random.randint(0,99)\n",
        "                rand_result2 = random.randint(0,99)\n",
        "\n",
        "                # Compute strategies and resulting state on success\n",
        "                if p1_optimal:\n",
        "                    s1 = agent1.optimal_strategies[(game_state[0],game_state[1])]\n",
        "                else:\n",
        "                    s1 = agent1.get_random_strategy(game_state[0])\n",
        "                d1 = g.states[game_state[0]-1].outcomes[s1]\n",
        "\n",
        "                if p2_optimal:\n",
        "                    s2 = agent2.optimal_strategies[(game_state[1],game_state[0])]\n",
        "                else:\n",
        "                    s2 = agent2.get_random_strategy(game_state[1])\n",
        "                d2 = g.states[game_state[1]-1].outcomes[s2]\n",
        "\n",
        "                # Compute success proability\n",
        "                p1_success = g.P(game_state[0],d1,s1,game_state[1],s2)\n",
        "                p2_success = g.P(game_state[1],d2,s2,game_state[0],s1)\n",
        "\n",
        "                # Compute outcome and rewards\n",
        "                new_state = (game_state[0],game_state[1])\n",
        "                r1 = g.R(game_state[0],game_state[0],s1,game_state[1],s2)\n",
        "                if rand_result1 < p1_success*100:\n",
        "                    r1 = g.R(game_state[0],d1,s1,game_state[1],s2)\n",
        "                    new_state = (d1,new_state[1])\n",
        "                r2 = g.R(game_state[1],game_state[1],s2,game_state[0],s1)\n",
        "                if g.IsHeadToHead(game_state[0],s1,game_state[1],s2):\n",
        "                    if rand_result1 >= 100 - p2_success*100:\n",
        "                        r2 = g.R(game_state[1],d2,s2,game_state[0],s1)\n",
        "                        new_state = (new_state[0],d2)\n",
        "                else:\n",
        "                    if rand_result2 < p2_success*100:\n",
        "                        r2 = g.R(game_state[1],d2,s2,game_state[0],s1)\n",
        "                        new_state = (new_state[0],d2)\n",
        "                game_state = (new_state[0],new_state[1])\n",
        "\n",
        "                p1_game_reward = p1_game_reward + r1\n",
        "                p2_game_reward = p2_game_reward + r2\n",
        "\n",
        "                # Check if there is a win\n",
        "                if game_state[0] == 5:\n",
        "                    q1 = True\n",
        "                if game_state[1] == 5:\n",
        "                    q2 = True\n",
        "\n",
        "                # Output and advance move\n",
        "                print(' --> ' + str(game_state), end = '')\n",
        "\n",
        "                moves = moves + 1\n",
        "            # (End game loop)\n",
        "\n",
        "            # Save game stats\n",
        "            if q1:\n",
        "                p1_wins = p1_wins+1\n",
        "            if q2:\n",
        "                p2_wins = p2_wins+1\n",
        "\n",
        "            p1_total_reward = p1_total_reward + p1_game_reward\n",
        "            p2_total_reward = p2_total_reward + p2_game_reward\n",
        "            total_moves = total_moves + moves\n",
        "            print(\" Reward = [\" + str(p1_game_reward) + \",\" + str(p2_game_reward) + \"]\")\n",
        "        # (End demo loop)\n",
        "\n",
        "        # Print outputs\n",
        "        average_moves_per_game = total_moves / num_games\n",
        "\n",
        "        print(g.players[0].name + \" wins: \" + str(p1_wins))\n",
        "        print(g.players[1].name + \" wins: \" + str(p2_wins))\n",
        "        print(\"Average moves per game: \" + str(average_moves_per_game))\n",
        "        print(\"Total reward for \" + g.players[0].name + \": \" + str(p1_total_reward))\n",
        "        print(\"Total reward for \" + g.players[1].name + \": \" + str(p2_total_reward))\n"
      ],
      "metadata": {
        "id": "3GpetI6DnnIb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class agent:\n",
        "    # Initializer\n",
        "    def __init__(self, game, player):\n",
        "      self.game = game\n",
        "      self.player = player\n",
        "      self.states = [s for s in game.states if s.name in game.players[player-1].states]\n",
        "      self.actions = {}\n",
        "      for state in self.states:\n",
        "          self.actions[state.name] = state.actions\n",
        "\n",
        "      self.q_values = {}  # Initialize Q-values to 0\n",
        "      for s in self.states:\n",
        "          for a in self.actions[s.name]:\n",
        "              self.q_values[(s.name, a)] = 0\n",
        "\n",
        "      self.gamma = 0.9  # Discount factor\n",
        "      self.alpha = 0.1  # Learning rate\n",
        "      self.epsilon = 0.1  # Exploration rate\n",
        "      self.optimal_strategies = {}\n",
        "\n",
        "\n",
        "    def get_random_strategy(self, state):\n",
        "        available_actions = self.actions[state]\n",
        "        if available_actions:\n",
        "            return random.choice(available_actions)\n",
        "        else:\n",
        "            return \"\"\n",
        "\n",
        "\n",
        "\n",
        "    # Train the agent using Q-learning\n",
        "    def train(self, num_episodes=10000, opponent=None):\n",
        "        for episode in range(num_episodes):\n",
        "            # Reset game state\n",
        "            state = self.game.players[self.player-1].start\n",
        "            opponent_state = self.game.players[2-self.player].start\n",
        "            done = False\n",
        "            while not done:\n",
        "                # Choose action using epsilon-greedy policy\n",
        "                if random.uniform(0, 1) < self.epsilon:\n",
        "                    action = random.choice(self.actions[state]) if self.actions[state] else None\n",
        "                else:\n",
        "                    state_actions = [(self.q_values.get((state, a), 0), a) for a in self.actions[state]]\n",
        "                    action = max(state_actions)[1] if state_actions else None\n",
        "\n",
        "                # Opponent chooses action using epsilon-greedy policy\n",
        "                if opponent is not None:\n",
        "                    if random.uniform(0, 1) < opponent.epsilon:\n",
        "                        opponent_actions = opponent.actions[opponent_state]\n",
        "                        opponent_action = random.choice(opponent_actions) if opponent_actions else None\n",
        "                    else:\n",
        "                        opp_state_actions = [(opponent.q_values.get((opponent_state, a), 0), a) for a in opponent.actions[opponent_state]]\n",
        "                        opponent_action = max(opp_state_actions)[1] if opp_state_actions else random.choice(opponent.actions[opponent_state])\n",
        "                else:\n",
        "                    opponent_actions = self.game.states[opponent_state-1].actions\n",
        "                    opponent_action = random.choice(opponent_actions) if opponent_actions else None\n",
        "\n",
        "                # Check if actions are available\n",
        "                if action is None or opponent_action is None:\n",
        "                    continue\n",
        "\n",
        "                # Take action and observe new state and reward\n",
        "                new_state = self.game.states[state-1].outcomes[action]\n",
        "                new_opponent_state = self.game.states[opponent_state-1].outcomes[opponent_action]\n",
        "                reward = self.game.R(state, new_state, action, opponent_state, opponent_action)\n",
        "\n",
        "                # Update Q-value\n",
        "                old_value = self.q_values.get((state, action), 0)\n",
        "                if new_state == 5:  # Terminal state\n",
        "                    next_max = 0\n",
        "                else:\n",
        "                    next_max = max([(self.q_values.get((new_state, a), 0), a) for a in self.actions[new_state]])[0]\n",
        "                self.q_values[(state, action)] = old_value + self.alpha * (reward + self.gamma * next_max - old_value)\n",
        "\n",
        "                # Update state\n",
        "                state = new_state\n",
        "                opponent_state = new_opponent_state\n",
        "\n",
        "                # Check if game is done\n",
        "                done = (state == 5) or (opponent_state == 5)\n",
        "\n",
        "        # Derive optimal strategies from Q-values\n",
        "        for s in self.states:\n",
        "            for opp_s in self.game.players[2-self.player].states:\n",
        "                state_actions = [(self.q_values.get((s.name, a), 0), a) for a in self.actions[s.name]]\n",
        "                self.optimal_strategies[(s.name, opp_s)] = max(state_actions)[1] if state_actions else random.choice(self.actions[s.name])\n",
        "\n",
        "    # Print the learned optimal strategies\n",
        "    def print_optimal_strategies(self):\n",
        "        print(f\"Optimal strategies for {self.game.players[self.player-1].name}:\")\n",
        "        for state, opp_state in self.optimal_strategies:\n",
        "            print(f\"State: {state}, Opponent State: {opp_state}, Optimal Action: {self.optimal_strategies[(state, opp_state)]}\")"
      ],
      "metadata": {
        "id": "LHgObqhDn2_9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Main module. Run this module to activate the program.\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "done = False\n",
        "\n",
        "# Here we initialize the preferences for the game.\n",
        "prefs = {\n",
        "    'R[H2H]':0.5,\n",
        "    'R[Win]':1,\n",
        "    'Discount':0.9, #changes by Rina Azoulay.\n",
        "    'P[(1,3)(r,l)|r]':2,\n",
        "    'P[(1,3)(r,l)|l]':3,\n",
        "    'P[1|u]':2,\n",
        "    'P[1|!u]':1,\n",
        "    'P[3|u]':1,\n",
        "    'P[3|!u]':2,\n",
        "    'P[(4,6)(r,l)|r]':2,\n",
        "    'P[(4,6)(r,l)|l]':3,\n",
        "    'P[(4,2)(r,u)|r]':3,\n",
        "    'P[(4,2)(r,u)|u]':1,\n",
        "    'P[(2,6)(u,l)|u]':1,\n",
        "    'P[(2,6)(u,l)|l]':3\n",
        "    }\n",
        "\n",
        "# The game and the agents are initialized to None to force the user to train\n",
        "# the agents before playing the demo.\n",
        "(g,a1,a2) = (None,None,None)\n",
        "\n",
        "while not done:\n",
        "    ui.print_menu()\n",
        "    choice = input(\"Your choice: \")\n",
        "    if choice == '1':                           # 1. Update preferences\n",
        "        prefs = ui.update_prefs(prefs)\n",
        "    elif choice == '2':                         # 2. Train agents\n",
        "        (g,a1,a2) = ui.train_agents(prefs)\n",
        "    elif choice == '3':                         # 3. Play demo\n",
        "        if g != None:\n",
        "            ui.play_demo(g,a1,a2)\n",
        "        else:\n",
        "            print(\"Cannot play demo without trained agents!\")\n",
        "    elif choice == '4':                         # 4. Exit\n",
        "        done = True\n",
        "    else:\n",
        "        print(\"Invalid choice!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPnVZdbunyhr",
        "outputId": "a52b732a-6ecc-4854-bd77-9fefdfa53d4e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------\n",
            "Menu\n",
            "------------------------------------------------\n",
            "    1. Change preferences\n",
            "    2. Train agents\n",
            "    3. Play demo\n",
            "    4. Exit\n",
            "------------------------------------------------\n",
            "Your choice: 2\n",
            "Optimal strategies for Mario:\n",
            "State: 1, Opponent State: 2, Optimal Action: up\n",
            "State: 1, Opponent State: 3, Optimal Action: up\n",
            "State: 1, Opponent State: 5, Optimal Action: up\n",
            "State: 1, Opponent State: 6, Optimal Action: up\n",
            "State: 2, Opponent State: 2, Optimal Action: up\n",
            "State: 2, Opponent State: 3, Optimal Action: up\n",
            "State: 2, Opponent State: 5, Optimal Action: up\n",
            "State: 2, Opponent State: 6, Optimal Action: up\n",
            "State: 4, Opponent State: 2, Optimal Action: right\n",
            "State: 4, Opponent State: 3, Optimal Action: right\n",
            "State: 4, Opponent State: 5, Optimal Action: right\n",
            "State: 4, Opponent State: 6, Optimal Action: right\n",
            "State: 5, Opponent State: 2, Optimal Action: \n",
            "State: 5, Opponent State: 3, Optimal Action: \n",
            "State: 5, Opponent State: 5, Optimal Action: \n",
            "State: 5, Opponent State: 6, Optimal Action: \n",
            "Optimal strategies for Bowser:\n",
            "State: 2, Opponent State: 1, Optimal Action: up\n",
            "State: 2, Opponent State: 2, Optimal Action: up\n",
            "State: 2, Opponent State: 4, Optimal Action: up\n",
            "State: 2, Opponent State: 5, Optimal Action: up\n",
            "State: 3, Opponent State: 1, Optimal Action: up\n",
            "State: 3, Opponent State: 2, Optimal Action: up\n",
            "State: 3, Opponent State: 4, Optimal Action: up\n",
            "State: 3, Opponent State: 5, Optimal Action: up\n",
            "State: 5, Opponent State: 1, Optimal Action: \n",
            "State: 5, Opponent State: 2, Optimal Action: \n",
            "State: 5, Opponent State: 4, Optimal Action: \n",
            "State: 5, Opponent State: 5, Optimal Action: \n",
            "State: 6, Opponent State: 1, Optimal Action: left\n",
            "State: 6, Opponent State: 2, Optimal Action: left\n",
            "State: 6, Opponent State: 4, Optimal Action: left\n",
            "State: 6, Opponent State: 5, Optimal Action: left\n",
            "------------------------------------------------\n",
            "Menu\n",
            "------------------------------------------------\n",
            "    1. Change preferences\n",
            "    2. Train agents\n",
            "    3. Play demo\n",
            "    4. Exit\n",
            "------------------------------------------------\n",
            "Your choice: 3\n",
            "How many games do you want to simulate? 3\n",
            "Do you want Mario to use optimal strategies (Y/N)? y\n",
            "Do you want Bowser to use optimal strategies (Y/N)? n\n",
            "1.(1, 3) --> (4, 2) --> (5, 2) Reward = [1,-1]\n",
            "2.(1, 3) --> (1, 3) --> (4, 2) --> (5, 2) Reward = [1,-1]\n",
            "3.(1, 3) --> (4, 2) --> (4, 5) Reward = [-1,1]\n",
            "Mario wins: 2\n",
            "Bowser wins: 1\n",
            "Average moves per game: 2.3333333333333335\n",
            "Total reward for Mario: 1\n",
            "Total reward for Bowser: -1\n",
            "------------------------------------------------\n",
            "Menu\n",
            "------------------------------------------------\n",
            "    1. Change preferences\n",
            "    2. Train agents\n",
            "    3. Play demo\n",
            "    4. Exit\n",
            "------------------------------------------------\n",
            "Your choice: 4\n"
          ]
        }
      ]
    }
  ]
}